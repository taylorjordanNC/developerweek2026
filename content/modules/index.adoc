# Fast, Cheap, and Accurate: Optimizing LLM Inference with vLLM and Quantization

Welcome to our 50-minute workshop for DeveloperWeek 2026. We're excited to introduce you to the world of vLLM and Quantized models. 

This repository contains the following content:

- Presentation slides to ramp you up on vLLM, quantized models, and inference optimization. 
- Modules that walk you through specific steps during the experience with your provided lab environments.

## Running this lab on your own hardware

### Prerequisites
- Compatible linux or MacOS operating system for vLLM
  - NOTE: For optimal performance, accelerator hardware is preferred but not required.
- vLLM installed
- Deployment of any vLLM-supported model
  - You may find a list of validated, quantized models on the [Red Hat AI public Hugging Face repository](https://huggingface.co/RedHatAI). 
- Python v3.10+

## Let's go!


