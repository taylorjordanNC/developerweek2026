# Module 1: Set up environment and chat with model

## Check version of vLLM inside of the provided RHAIIS image 

[source,sh,role="execute"]
----
podman exec -it rhaiis vllm --version
----

## Check models served by vLLM

[source,sh,role="execute"]
----
curl http://localhost:8000/v1/models
----

## Install OpenAI python library

[source,sh,role="execute"]
----
pip install openai
----

### What it is

When you run `pip install openai`, you are installing a specialized client interface that allows your Python code to communicate with AI models via the OpenAI protocol. Since vLLM and RHAIIS are designed to "speak" this same protocol, this library acts as the perfect bridge.

### What we're actually installing

It isn't just one single file; it's a collection of tools and "dependencies" that work together to handle the complex networking required for AI:

* The OpenAI Wrapper: This provides the high-level commands like client.chat.completions.create(). It turns your simple Python code into the complex JSON data the server needs.

* HTTPX: This is the "engine" that handles the actual internet connection. It is responsible for sending your request and waiting for the AI to "talk" back.

* Pydantic: A powerful tool used for data validation. It ensures that the response coming back from your model is formatted correctly so your script doesn't crash.

* Typing Extensions: This adds support for modern Python type-hinting, which helps developers see errors in their code before they even run it.

## Setup a custom in-terminal chat client

### Create a new file called chat.py

[source,sh,role="execute"]
----
vim chat.py
----

Paste the following code into your new file:

[source,python,role="execute"]
----
from openai import OpenAI
import sys

# Connect to your local RHAIIS server
client = OpenAI(base_url="http://localhost:8000/v1", api_key="none")

# Fetch the model name automatically
model_name = client.models.list().data[0].id

# Define the System Prompt
system_message = {
    "role": "system", 
    "content": "You are a helpful Red Hat technical expert. You give concise, accurate answers and always explain concepts in the context of RHEL and OpenShift."
}

print(f"--- Chatting with {model_name} (Type 'exit' to quit) ---")

history = [system_message] # Initialize history with the system prompt

while True:
    user_input = input("\nYou: ")
    if user_input.lower() in ["exit", "quit"]:
        break

    history.append({"role": "user", "content": user_input})
    
    # Request streaming completion
    stream = client.chat.completions.create(
        model=model_name,
        messages=history,
        stream=True
    )

    print("AI: ", end="", flush=True)
    assistant_text = ""
    
    # Force-flush each token to bypass terminal buffering
    for chunk in stream:
        content = chunk.choices[0].delta.content
        if content:
            print(content, end="", flush=True)
            assistant_text += content
    
    print()
    history.append({"role": "assistant", "content": assistant_text})
----

### Close and save vim editor

Press kbd:[Esc] to edit input mode. Then save:

[source,sh,role="execute"]
----
:wq!
----

### Run the chat with your model

[source,sh,role="execute"]
----
python chat.py
----

Interact with the model, ask it a few basic questions.

### Check gpu utilization during chat

Open a second terminal window and ssh into the provided workshop instance. Run the following command:

[source,sh,role="execute"]
----
nvtop
----

Now that we've set up a basic chat with our model and see how to view gpu utilization, let's run our first benchmark.