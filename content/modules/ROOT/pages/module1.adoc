# Module 1: Environment Setup and Interactive Chat

**What you'll accomplish:** Connect to a quantized Gemma model running in vLLM, verify it's working, and build a simple chat interface.

First, let's set up our local terminal

## ssh to the system

Using the ssh command given at the workshop link, ssh to the provided linux system. Open up two terminal windows.

Next, confirm the vLLM server is running with your quantized model.

## Check version of vLLM inside of the provided RHAIIS image 

[source,sh,role="execute"]
----
podman exec -it rhaiis vllm --version
----

## Check models served by vLLM

[source,sh,role="execute"]
----
curl http://localhost:8000/v1/models
----

**Expected output:** You should see `RedHatAI/gemma-3-1b-it-quantized.w8a8` in the response. This confirms vLLM is serving your quantized model via an OpenAI-compatible API.

## Install OpenAI python library

[source,sh,role="execute"]
----
pip install openai
----

### Why OpenAI library?

vLLM speaks the OpenAI API protocol, so we use the `openai` Python library as our client. This gives us a standardized way to send prompts and receive responses, just like you would with ChatGPT's API.

## Setup a custom in-terminal chat client

### Create a new file called chat.py

[source,sh,role="execute"]
----
vim chat.py
----

Paste the following code into your new file:

[source,python,role="execute"]
----
from openai import OpenAI
import sys

# Connect to your local RHAIIS server
client = OpenAI(base_url="http://localhost:8000/v1", api_key="none")

# Fetch the model name automatically
model_name = client.models.list().data[0].id

# Define the System Prompt
system_message = {
    "role": "system", 
    "content": "You are a helpful Red Hat technical expert. You give concise, accurate answers and always explain concepts in the context of RHEL and OpenShift."
}

print(f"--- Chatting with {model_name} (Type 'exit' to quit) ---")

history = [system_message] # Initialize history with the system prompt

while True:
    user_input = input("\nYou: ")
    if user_input.lower() in ["exit", "quit"]:
        break

    history.append({"role": "user", "content": user_input})
    
    # Request streaming completion
    stream = client.chat.completions.create(
        model=model_name,
        messages=history,
        stream=True
    )

    print("AI: ", end="", flush=True)
    assistant_text = ""
    
    # Force-flush each token to bypass terminal buffering
    for chunk in stream:
        content = chunk.choices[0].delta.content
        if content:
            print(content, end="", flush=True)
            assistant_text += content
    
    print()
    history.append({"role": "assistant", "content": assistant_text})
----

### Close and save vim editor

Press kbd:[Esc] to edit input mode. Then save:

[source,sh,role="execute"]
----
:wq!
----

### Run the chat with your model

[source,sh,role="execute"]
----
python chat.py
----

**Try these questions:**
- "What is quantization?"
- "Explain Red Hat Enterprise Linux in one sentence"
- "What's the difference between vLLM and standard inference?"

Watch how the model streams responses token-by-token.

### Check gpu utilization during chat

Open a second terminal window and ssh into the provided workshop instance. Run the following command:

[source,sh,role="execute"]
----
nvtop
----

**What to observe:** GPU memory usage and utilization spikes when generating responses.

Now that we've set up a basic chat with our model and see how to view gpu utilization, let's run our first benchmark.